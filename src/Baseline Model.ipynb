{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77d36cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5043cc78",
   "metadata": {},
   "source": [
    "## 1. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b37ecab-b186-4d34-adef-e557f3715cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets given\n",
    "df_movie_details = pd.read_json(\"../data/IMDB_movie_details.json\", lines = True)\n",
    "df_reviews = pd.read_json(\"../data/IMDB_reviews.json\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88639b81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1572 entries, 0 to 1571\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   movie_id       1572 non-null   object \n",
      " 1   plot_summary   1572 non-null   object \n",
      " 2   duration       1572 non-null   object \n",
      " 3   genre          1572 non-null   object \n",
      " 4   rating         1572 non-null   float64\n",
      " 5   release_date   1572 non-null   object \n",
      " 6   plot_synopsis  1572 non-null   object \n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 86.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_movie_details.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dccfc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 573913 entries, 0 to 573912\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   review_date     573913 non-null  object\n",
      " 1   movie_id        573913 non-null  object\n",
      " 2   user_id         573913 non-null  object\n",
      " 3   is_spoiler      573913 non-null  bool  \n",
      " 4   review_text     573913 non-null  object\n",
      " 5   rating          573913 non-null  int64 \n",
      " 6   review_summary  573913 non-null  object\n",
      "dtypes: bool(1), int64(1), object(5)\n",
      "memory usage: 26.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "592b01a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.read_pickle(\"../data/tokenized_reviews.pkl.gz\", compression = 'gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62430f7",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87377180",
   "metadata": {},
   "source": [
    "### Tokenize review texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad9ceae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZING WILL TAKE ~15 MINUTES\n",
    "# If you want to save after tokenizing feel free to do so to save time in tokenizing again\n",
    "df_reviews['tokenized_summary'] = list(map(word_tokenize, df_reviews['review_summary']))\n",
    "df_reviews['tokenized_reviews'] = list(map(word_tokenize, df_reviews['review_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7712e14",
   "metadata": {},
   "source": [
    "### Removing stop words and punctuations from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e85064f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stop words\n",
    "stop_words_and_punctuations = set(stopwords.words('english') + list(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9701583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words and punctuations from the tokenized list\n",
    "df_reviews['tokenized_summary'] = list(map(lambda x: [word.lower() for word in x if word.lower() not in stop_words_and_punctuations], df_reviews['tokenized_summary']))\n",
    "df_reviews['tokenized_reviews'] = list(map(lambda x: [word.lower() for word in x if word.lower() not in stop_words_and_punctuations], df_reviews['tokenized_reviews']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9702f68",
   "metadata": {},
   "source": [
    "### Stemming or Lemmatisation -- (to be implemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b8170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem or lemmatise words\n",
    "stemmer = PorterStemmer()\n",
    "df_reviews['tokenized_summary'] = list(map(lambda x: [stemmer.stem(word) for word in x], df_reviews['tokenized_summary']))\n",
    "df_reviews['tokenized_reviews'] = list(map(lambda x: [stemmer.stem(word) for word in x], df_reviews['tokenized_reviews']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1fc0f5",
   "metadata": {},
   "source": [
    "### Save the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "107773b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save changes made to original dataset to save time tokenizing etc\n",
    "df_reviews.to_pickle(\"../data/cleaned_reviews.pkl.gz\", compression = 'gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2babeb1",
   "metadata": {},
   "source": [
    "## 3. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7048bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously saved dataset\n",
    "df_reviews = pd.read_pickle(\"../data/cleaned_reviews.pkl.gz\", compression = 'gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4369bd5",
   "metadata": {},
   "source": [
    "### Transform texts to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7599a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews[\"text_tokenized\"] = list(map(lambda x: ' '.join(x), df_reviews['tokenized_reviews']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fcc61c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform(df_reviews[\"text_tokenized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4ca2e3",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0750ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(bow, df_reviews.loc[:, 'is_spoiler'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdc53557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_model = LogisticRegression(max_iter = 1e3)\n",
    "logistic_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e28cf9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7638761837554342"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d48993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
