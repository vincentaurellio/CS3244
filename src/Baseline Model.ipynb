{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77d36cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5043cc78",
   "metadata": {},
   "source": [
    "## 1. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b37ecab-b186-4d34-adef-e557f3715cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets given\n",
    "df_movie_details = pd.read_json(\"../data/IMDB_movie_details.json\", lines = True)\n",
    "df_reviews = pd.read_json(\"../data/IMDB_reviews.json\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ef41315",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.read_pickle(\"../data/tokenized_reviews.pkl.gz\", compression = 'gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62430f7",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87377180",
   "metadata": {},
   "source": [
    "### Tokenize review texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad9ceae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZING WILL TAKE ~15 MINUTES\n",
    "# If you want to save after tokenizing feel free to do so to save time in tokenizing again\n",
    "df_reviews['tokenized_summary'] = list(map(word_tokenize, df_reviews['review_summary']))\n",
    "df_reviews['tokenized_reviews'] = list(map(word_tokenize, df_reviews['review_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7712e14",
   "metadata": {},
   "source": [
    "### Removing stop words and punctuations from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e85064f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stop words\n",
    "stop_words_and_punctuations = set(stopwords.words('english') + list(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9701583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words and punctuations from the tokenized list\n",
    "df_reviews['tokenized_summary'] = list(map(lambda x: [word.lower() for word in x if word.lower() not in stop_words_and_punctuations], df_reviews['tokenized_summary']))\n",
    "df_reviews['tokenized_reviews'] = list(map(lambda x: [word.lower() for word in x if word.lower() not in stop_words_and_punctuations], df_reviews['tokenized_reviews']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9702f68",
   "metadata": {},
   "source": [
    "### Stemming or Lemmatisation -- (to be implemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b8170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem or lemmatise words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4369bd5",
   "metadata": {},
   "source": [
    "### Transform texts to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b447aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e1fc0f5",
   "metadata": {},
   "source": [
    "### Save the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107773b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save changes made to original dataset to save time tokenizing etc\n",
    "df_reviews.to_pickle(\"../data/cleaned_reviews.pkl.gz\", compression = 'gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2babeb1",
   "metadata": {},
   "source": [
    "## 3. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7048bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously saved dataset\n",
    "df_reviews = pd.read_pickle(\"../data/cleaned_reviews.pkl.gz\", compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0750ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_reviews.loc[:, df_reviews.columns != 'is_spoiler'], df_reviews.loc[:, 'is_spoiler'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc53557",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegression(max_iter = 1e3)\n",
    "# To be implemented\n",
    "# logistic_model.fit(X_train['tokenized_reviews'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e28cf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbd15d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
