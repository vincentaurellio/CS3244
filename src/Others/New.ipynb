{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lakbo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lakbo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import bigrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearnex import patch_sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "patch_sklearn()\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"movie_details = pd.read_json('../data/IMDB_movie_details.json', lines = True)\\nimdb_reviews = pd.read_json('../data/IMDB_reviews.json', lines = True)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing\n",
    "\"\"\"movie_details = pd.read_json('../data/IMDB_movie_details.json', lines = True)\n",
    "imdb_reviews = pd.read_json('../data/IMDB_reviews.json', lines = True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"no_summary = movie_details[movie_details['plot_summary'].isna()]\\nno_summary\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for na\n",
    "\"\"\"no_summary = movie_details[movie_details['plot_summary'].isna()]\n",
    "no_summary\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"no_synopsis = movie_details[movie_details['plot_synopsis'].isna()]\\nno_synopsis\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for na\n",
    "\"\"\"no_synopsis = movie_details[movie_details['plot_synopsis'].isna()]\n",
    "no_synopsis\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"no_review = imdb_reviews[imdb_reviews['review_text'].isna()]\\nno_review\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for na\n",
    "\"\"\"no_review = imdb_reviews[imdb_reviews['review_text'].isna()]\n",
    "no_review\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def lowercase(dataframe,columnname):\\n    lowercase_dataframe = dataframe[columnname].apply(lambda x: x.lower())\\n    return lowercase_dataframe\\n\\ndef remove_special_characters(dataframe,columnname):\\n    dataframe_no_special_characters = dataframe[columnname].replace(r'[^A-Za-z0-9 ]+', '', regex=True)\\n    return dataframe_no_special_characters\\n\\ndef tokenize_words(dataframe,columnname):\\n    dataframe_tokenized_texts= dataframe[columnname].apply(lambda x: word_tokenize(x) )\\n    return dataframe_tokenized_texts\\n\\ndef remove_stop_words(dataframe,columnname):\\n    stop = stopwords.words('english')\\n    dataframe_no_stop_words= dataframe[columnname].apply(lambda x: [item for item in x if item not in stop])\\n    return dataframe_no_stop_words\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#functions for preprocessing\n",
    "\"\"\"def lowercase(dataframe,columnname):\n",
    "    lowercase_dataframe = dataframe[columnname].apply(lambda x: x.lower())\n",
    "    return lowercase_dataframe\n",
    "\n",
    "def remove_special_characters(dataframe,columnname):\n",
    "    dataframe_no_special_characters = dataframe[columnname].replace(r'[^A-Za-z0-9 ]+', '', regex=True)\n",
    "    return dataframe_no_special_characters\n",
    "\n",
    "def tokenize_words(dataframe,columnname):\n",
    "    dataframe_tokenized_texts= dataframe[columnname].apply(lambda x: word_tokenize(x) )\n",
    "    return dataframe_tokenized_texts\n",
    "\n",
    "def remove_stop_words(dataframe,columnname):\n",
    "    stop = stopwords.words('english')\n",
    "    dataframe_no_stop_words= dataframe[columnname].apply(lambda x: [item for item in x if item not in stop])\n",
    "    return dataframe_no_stop_words\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"imdb_reviews = imdb_reviews.head()\\nimdb_reviews['review_text'] = lowercase(imdb_reviews,'review_text')\\nimdb_reviews['review_text'] = remove_special_characters(imdb_reviews,'review_text')\\nimdb_reviews['review_text'] = tokenize_words(imdb_reviews,'review_text')\\nimdb_reviews['review_text'] = remove_stop_words(imdb_reviews,'review_text')\\nimdb_reviews\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Processing of data\n",
    "\"\"\"imdb_reviews = imdb_reviews.head()\n",
    "imdb_reviews['review_text'] = lowercase(imdb_reviews,'review_text')\n",
    "imdb_reviews['review_text'] = remove_special_characters(imdb_reviews,'review_text')\n",
    "imdb_reviews['review_text'] = tokenize_words(imdb_reviews,'review_text')\n",
    "imdb_reviews['review_text'] = remove_stop_words(imdb_reviews,'review_text')\n",
    "imdb_reviews\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"imdb_reviews['review_text'] = lowercase(imdb_reviews,'review_text')\\n#remove special characters\\nimdb_reviews['review_text'] = remove_special_characters(imdb_reviews,'review_text')\\n#tokenize text 15min\\nimdb_reviews['review_text'] = tokenize_words(imdb_reviews,'review_text')\\n#remove stop words 8min\\nimdb_reviews['review_text'] = remove_stop_words(imdb_reviews,'review_text')\\nimdb_reviews['is_spoiler'] = imdb_reviews['is_spoiler'].map({True: 1, False: 0})\\nimdb_reviews.head()\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to lowercase\n",
    "\"\"\"imdb_reviews['review_text'] = lowercase(imdb_reviews,'review_text')\n",
    "#remove special characters\n",
    "imdb_reviews['review_text'] = remove_special_characters(imdb_reviews,'review_text')\n",
    "#tokenize text 15min\n",
    "imdb_reviews['review_text'] = tokenize_words(imdb_reviews,'review_text')\n",
    "#remove stop words 8min\n",
    "imdb_reviews['review_text'] = remove_stop_words(imdb_reviews,'review_text')\n",
    "imdb_reviews['is_spoiler'] = imdb_reviews['is_spoiler'].map({True: 1, False: 0})\n",
    "imdb_reviews.head()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"imdb_reviews['bigrams'] = imdb_reviews['review_text'].apply(lambda x: list(bigrams(x)))\\nimdb_reviews['bigram_text'] = imdb_reviews['bigrams'].apply(lambda x: ' '.join([' '.join(bigram) for bigram in x]))\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bigrams and bigram text\n",
    "\"\"\"imdb_reviews['bigrams'] = imdb_reviews['review_text'].apply(lambda x: list(bigrams(x)))\n",
    "imdb_reviews['bigram_text'] = imdb_reviews['bigrams'].apply(lambda x: ' '.join([' '.join(bigram) for bigram in x]))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From csv file with bigrams text\n",
    "imdb_reviews_bigrams = pd.read_csv('../data/imdb_reviews_nostop_bigrams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review_date</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>is_spoiler</th>\n",
       "      <th>review_text</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_summary</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>bigram_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10 February 2006</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>ur1898687</td>\n",
       "      <td>1</td>\n",
       "      <td>['oscar', 'year', 'shawshank', 'redemption', '...</td>\n",
       "      <td>10</td>\n",
       "      <td>A classic piece of unforgettable film-making.</td>\n",
       "      <td>[('oscar', 'year'), ('year', 'shawshank'), ('s...</td>\n",
       "      <td>oscar year year shawshank shawshank redemption...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6 September 2000</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>ur0842118</td>\n",
       "      <td>1</td>\n",
       "      <td>['shawshank', 'redemption', 'without', 'doubt'...</td>\n",
       "      <td>10</td>\n",
       "      <td>Simply amazing. The best film of the 90's.</td>\n",
       "      <td>[('shawshank', 'redemption'), ('redemption', '...</td>\n",
       "      <td>shawshank redemption redemption without withou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3 August 2001</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>ur1285640</td>\n",
       "      <td>1</td>\n",
       "      <td>['believe', 'film', 'best', 'story', 'ever', '...</td>\n",
       "      <td>8</td>\n",
       "      <td>The best story ever told on film</td>\n",
       "      <td>[('believe', 'film'), ('film', 'best'), ('best...</td>\n",
       "      <td>believe film film best best story story ever e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1 September 2002</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>ur1003471</td>\n",
       "      <td>1</td>\n",
       "      <td>['yes', 'spoilers', 'herethis', 'film', 'emoti...</td>\n",
       "      <td>10</td>\n",
       "      <td>Busy dying or busy living?</td>\n",
       "      <td>[('yes', 'spoilers'), ('spoilers', 'herethis')...</td>\n",
       "      <td>yes spoilers spoilers herethis herethis film f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20 May 2004</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>ur0226855</td>\n",
       "      <td>1</td>\n",
       "      <td>['heart', 'extraordinary', 'movie', 'brilliant...</td>\n",
       "      <td>8</td>\n",
       "      <td>Great story, wondrously told and acted</td>\n",
       "      <td>[('heart', 'extraordinary'), ('extraordinary',...</td>\n",
       "      <td>heart extraordinary extraordinary movie movie ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       review_date   movie_id    user_id  is_spoiler  \\\n",
       "0           0  10 February 2006  tt0111161  ur1898687           1   \n",
       "1           1  6 September 2000  tt0111161  ur0842118           1   \n",
       "2           2     3 August 2001  tt0111161  ur1285640           1   \n",
       "3           3  1 September 2002  tt0111161  ur1003471           1   \n",
       "4           4       20 May 2004  tt0111161  ur0226855           1   \n",
       "\n",
       "                                         review_text  rating  \\\n",
       "0  ['oscar', 'year', 'shawshank', 'redemption', '...      10   \n",
       "1  ['shawshank', 'redemption', 'without', 'doubt'...      10   \n",
       "2  ['believe', 'film', 'best', 'story', 'ever', '...       8   \n",
       "3  ['yes', 'spoilers', 'herethis', 'film', 'emoti...      10   \n",
       "4  ['heart', 'extraordinary', 'movie', 'brilliant...       8   \n",
       "\n",
       "                                  review_summary  \\\n",
       "0  A classic piece of unforgettable film-making.   \n",
       "1     Simply amazing. The best film of the 90's.   \n",
       "2               The best story ever told on film   \n",
       "3                     Busy dying or busy living?   \n",
       "4         Great story, wondrously told and acted   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0  [('oscar', 'year'), ('year', 'shawshank'), ('s...   \n",
       "1  [('shawshank', 'redemption'), ('redemption', '...   \n",
       "2  [('believe', 'film'), ('film', 'best'), ('best...   \n",
       "3  [('yes', 'spoilers'), ('spoilers', 'herethis')...   \n",
       "4  [('heart', 'extraordinary'), ('extraordinary',...   \n",
       "\n",
       "                                         bigram_text  \n",
       "0  oscar year year shawshank shawshank redemption...  \n",
       "1  shawshank redemption redemption without withou...  \n",
       "2  believe film film best best story story ever e...  \n",
       "3  yes spoilers spoilers herethis herethis film f...  \n",
       "4  heart extraordinary extraordinary movie movie ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_reviews_bigrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review_date</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>is_spoiler</th>\n",
       "      <th>review_text</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_summary</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>bigram_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30552</th>\n",
       "      <td>30552</td>\n",
       "      <td>24 December 2017</td>\n",
       "      <td>tt0120586</td>\n",
       "      <td>ur64553360</td>\n",
       "      <td>0</td>\n",
       "      <td>['minxfyicbcfrxesydrfvbhjnmkjiuytfrdesdcfvghju...</td>\n",
       "      <td>10</td>\n",
       "      <td>the Best</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33433</th>\n",
       "      <td>33433</td>\n",
       "      <td>27 December 2017</td>\n",
       "      <td>tt0027977</td>\n",
       "      <td>ur83694676</td>\n",
       "      <td>1</td>\n",
       "      <td>['nice']</td>\n",
       "      <td>9</td>\n",
       "      <td>Very good</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269257</th>\n",
       "      <td>269257</td>\n",
       "      <td>20 December 2017</td>\n",
       "      <td>tt2239822</td>\n",
       "      <td>ur83454181</td>\n",
       "      <td>0</td>\n",
       "      <td>['greatgreatgreatgreatgreatgreatgreatgreatgrea...</td>\n",
       "      <td>9</td>\n",
       "      <td>great</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487228</th>\n",
       "      <td>487228</td>\n",
       "      <td>15 October 1998</td>\n",
       "      <td>tt0102685</td>\n",
       "      <td>ur0111020</td>\n",
       "      <td>0</td>\n",
       "      <td>['headacheinducing']</td>\n",
       "      <td>1</td>\n",
       "      <td>Two words:</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0       review_date   movie_id     user_id  is_spoiler  \\\n",
       "30552        30552  24 December 2017  tt0120586  ur64553360           0   \n",
       "33433        33433  27 December 2017  tt0027977  ur83694676           1   \n",
       "269257      269257  20 December 2017  tt2239822  ur83454181           0   \n",
       "487228      487228   15 October 1998  tt0102685   ur0111020           0   \n",
       "\n",
       "                                              review_text  rating  \\\n",
       "30552   ['minxfyicbcfrxesydrfvbhjnmkjiuytfrdesdcfvghju...      10   \n",
       "33433                                            ['nice']       9   \n",
       "269257  ['greatgreatgreatgreatgreatgreatgreatgreatgrea...       9   \n",
       "487228                               ['headacheinducing']       1   \n",
       "\n",
       "       review_summary bigrams bigram_text  \n",
       "30552        the Best      []         NaN  \n",
       "33433       Very good      []         NaN  \n",
       "269257          great      []         NaN  \n",
       "487228     Two words:      []         NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for na\n",
    "imdb_reviews_bigrams[imdb_reviews_bigrams['bigram_text'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review_date</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>is_spoiler</th>\n",
       "      <th>review_text</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_summary</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>bigram_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10 February 2006</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>ur1898687</td>\n",
       "      <td>1</td>\n",
       "      <td>['oscar', 'year', 'shawshank', 'redemption', '...</td>\n",
       "      <td>10</td>\n",
       "      <td>A classic piece of unforgettable film-making.</td>\n",
       "      <td>[('oscar', 'year'), ('year', 'shawshank'), ('s...</td>\n",
       "      <td>oscar year year shawshank shawshank redemption...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6 September 2000</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>ur0842118</td>\n",
       "      <td>1</td>\n",
       "      <td>['shawshank', 'redemption', 'without', 'doubt'...</td>\n",
       "      <td>10</td>\n",
       "      <td>Simply amazing. The best film of the 90's.</td>\n",
       "      <td>[('shawshank', 'redemption'), ('redemption', '...</td>\n",
       "      <td>shawshank redemption redemption without withou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3 August 2001</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>ur1285640</td>\n",
       "      <td>1</td>\n",
       "      <td>['believe', 'film', 'best', 'story', 'ever', '...</td>\n",
       "      <td>8</td>\n",
       "      <td>The best story ever told on film</td>\n",
       "      <td>[('believe', 'film'), ('film', 'best'), ('best...</td>\n",
       "      <td>believe film film best best story story ever e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1 September 2002</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>ur1003471</td>\n",
       "      <td>1</td>\n",
       "      <td>['yes', 'spoilers', 'herethis', 'film', 'emoti...</td>\n",
       "      <td>10</td>\n",
       "      <td>Busy dying or busy living?</td>\n",
       "      <td>[('yes', 'spoilers'), ('spoilers', 'herethis')...</td>\n",
       "      <td>yes spoilers spoilers herethis herethis film f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20 May 2004</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>ur0226855</td>\n",
       "      <td>1</td>\n",
       "      <td>['heart', 'extraordinary', 'movie', 'brilliant...</td>\n",
       "      <td>8</td>\n",
       "      <td>Great story, wondrously told and acted</td>\n",
       "      <td>[('heart', 'extraordinary'), ('extraordinary',...</td>\n",
       "      <td>heart extraordinary extraordinary movie movie ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       review_date   movie_id    user_id  is_spoiler  \\\n",
       "0           0  10 February 2006  tt0111161  ur1898687           1   \n",
       "1           1  6 September 2000  tt0111161  ur0842118           1   \n",
       "2           2     3 August 2001  tt0111161  ur1285640           1   \n",
       "3           3  1 September 2002  tt0111161  ur1003471           1   \n",
       "4           4       20 May 2004  tt0111161  ur0226855           1   \n",
       "\n",
       "                                         review_text  rating  \\\n",
       "0  ['oscar', 'year', 'shawshank', 'redemption', '...      10   \n",
       "1  ['shawshank', 'redemption', 'without', 'doubt'...      10   \n",
       "2  ['believe', 'film', 'best', 'story', 'ever', '...       8   \n",
       "3  ['yes', 'spoilers', 'herethis', 'film', 'emoti...      10   \n",
       "4  ['heart', 'extraordinary', 'movie', 'brilliant...       8   \n",
       "\n",
       "                                  review_summary  \\\n",
       "0  A classic piece of unforgettable film-making.   \n",
       "1     Simply amazing. The best film of the 90's.   \n",
       "2               The best story ever told on film   \n",
       "3                     Busy dying or busy living?   \n",
       "4         Great story, wondrously told and acted   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0  [('oscar', 'year'), ('year', 'shawshank'), ('s...   \n",
       "1  [('shawshank', 'redemption'), ('redemption', '...   \n",
       "2  [('believe', 'film'), ('film', 'best'), ('best...   \n",
       "3  [('yes', 'spoilers'), ('spoilers', 'herethis')...   \n",
       "4  [('heart', 'extraordinary'), ('extraordinary',...   \n",
       "\n",
       "                                         bigram_text  \n",
       "0  oscar year year shawshank shawshank redemption...  \n",
       "1  shawshank redemption redemption without withou...  \n",
       "2  believe film film best best story story ever e...  \n",
       "3  yes spoilers spoilers herethis herethis film f...  \n",
       "4  heart extraordinary extraordinary movie movie ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove na data\n",
    "imdb_reviews_bigrams = imdb_reviews_bigrams.dropna(subset=['bigram_text'])\n",
    "imdb_reviews_bigrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize bigram text\n",
    "vectorizer = CountVectorizer()\n",
    "matrix = vectorizer.fit_transform(imdb_reviews_bigrams['bigram_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review_date</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>is_spoiler</th>\n",
       "      <th>review_text</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_summary</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>bigram_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10 February 2006</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>ur1898687</td>\n",
       "      <td>1</td>\n",
       "      <td>['oscar', 'year', 'shawshank', 'redemption', '...</td>\n",
       "      <td>10</td>\n",
       "      <td>A classic piece of unforgettable film-making.</td>\n",
       "      <td>[('oscar', 'year'), ('year', 'shawshank'), ('s...</td>\n",
       "      <td>oscar year year shawshank shawshank redemption...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6 September 2000</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>ur0842118</td>\n",
       "      <td>1</td>\n",
       "      <td>['shawshank', 'redemption', 'without', 'doubt'...</td>\n",
       "      <td>10</td>\n",
       "      <td>Simply amazing. The best film of the 90's.</td>\n",
       "      <td>[('shawshank', 'redemption'), ('redemption', '...</td>\n",
       "      <td>shawshank redemption redemption without withou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3 August 2001</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>ur1285640</td>\n",
       "      <td>1</td>\n",
       "      <td>['believe', 'film', 'best', 'story', 'ever', '...</td>\n",
       "      <td>8</td>\n",
       "      <td>The best story ever told on film</td>\n",
       "      <td>[('believe', 'film'), ('film', 'best'), ('best...</td>\n",
       "      <td>believe film film best best story story ever e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1 September 2002</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>ur1003471</td>\n",
       "      <td>1</td>\n",
       "      <td>['yes', 'spoilers', 'herethis', 'film', 'emoti...</td>\n",
       "      <td>10</td>\n",
       "      <td>Busy dying or busy living?</td>\n",
       "      <td>[('yes', 'spoilers'), ('spoilers', 'herethis')...</td>\n",
       "      <td>yes spoilers spoilers herethis herethis film f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20 May 2004</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>ur0226855</td>\n",
       "      <td>1</td>\n",
       "      <td>['heart', 'extraordinary', 'movie', 'brilliant...</td>\n",
       "      <td>8</td>\n",
       "      <td>Great story, wondrously told and acted</td>\n",
       "      <td>[('heart', 'extraordinary'), ('extraordinary',...</td>\n",
       "      <td>heart extraordinary extraordinary movie movie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573908</th>\n",
       "      <td>573908</td>\n",
       "      <td>8 August 1999</td>\n",
       "      <td>tt0139239</td>\n",
       "      <td>ur0100166</td>\n",
       "      <td>0</td>\n",
       "      <td>['go', 'wise', 'fast', 'pure', 'entertainment'...</td>\n",
       "      <td>10</td>\n",
       "      <td>The best teen movie of the nineties</td>\n",
       "      <td>[('go', 'wise'), ('wise', 'fast'), ('fast', 'p...</td>\n",
       "      <td>go wise wise fast fast pure pure entertainment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573909</th>\n",
       "      <td>573909</td>\n",
       "      <td>31 July 1999</td>\n",
       "      <td>tt0139239</td>\n",
       "      <td>ur0021767</td>\n",
       "      <td>0</td>\n",
       "      <td>['well', 'shall', 'say', 'ones', 'fun', 'rate'...</td>\n",
       "      <td>9</td>\n",
       "      <td>Go - see the movie</td>\n",
       "      <td>[('well', 'shall'), ('shall', 'say'), ('say', ...</td>\n",
       "      <td>well shall shall say say ones ones fun fun rat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573910</th>\n",
       "      <td>573910</td>\n",
       "      <td>20 July 1999</td>\n",
       "      <td>tt0139239</td>\n",
       "      <td>ur0392750</td>\n",
       "      <td>0</td>\n",
       "      <td>['go', 'best', 'movie', 'ever', 'seen', 'ive',...</td>\n",
       "      <td>10</td>\n",
       "      <td>It's the best movie I've ever seen</td>\n",
       "      <td>[('go', 'best'), ('best', 'movie'), ('movie', ...</td>\n",
       "      <td>go best best movie movie ever ever seen seen i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573911</th>\n",
       "      <td>573911</td>\n",
       "      <td>11 June 1999</td>\n",
       "      <td>tt0139239</td>\n",
       "      <td>ur0349105</td>\n",
       "      <td>0</td>\n",
       "      <td>['call', '1999', 'teenage', 'version', 'pulp',...</td>\n",
       "      <td>3</td>\n",
       "      <td>Haven't we seen this before?</td>\n",
       "      <td>[('call', '1999'), ('1999', 'teenage'), ('teen...</td>\n",
       "      <td>call 1999 1999 teenage teenage version version...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573912</th>\n",
       "      <td>573912</td>\n",
       "      <td>3 May 1999</td>\n",
       "      <td>tt0139239</td>\n",
       "      <td>ur0156431</td>\n",
       "      <td>0</td>\n",
       "      <td>['movie', 'made', 'doubt', 'sucker', 'familyre...</td>\n",
       "      <td>2</td>\n",
       "      <td>Go doesn't go anywhere</td>\n",
       "      <td>[('movie', 'made'), ('made', 'doubt'), ('doubt...</td>\n",
       "      <td>movie made made doubt doubt sucker sucker fami...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>573909 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0       review_date   movie_id    user_id  is_spoiler  \\\n",
       "0                0  10 February 2006  tt0111161  ur1898687           1   \n",
       "1                1  6 September 2000  tt0111161  ur0842118           1   \n",
       "2                2     3 August 2001  tt0111161  ur1285640           1   \n",
       "3                3  1 September 2002  tt0111161  ur1003471           1   \n",
       "4                4       20 May 2004  tt0111161  ur0226855           1   \n",
       "...            ...               ...        ...        ...         ...   \n",
       "573908      573908     8 August 1999  tt0139239  ur0100166           0   \n",
       "573909      573909      31 July 1999  tt0139239  ur0021767           0   \n",
       "573910      573910      20 July 1999  tt0139239  ur0392750           0   \n",
       "573911      573911      11 June 1999  tt0139239  ur0349105           0   \n",
       "573912      573912        3 May 1999  tt0139239  ur0156431           0   \n",
       "\n",
       "                                              review_text  rating  \\\n",
       "0       ['oscar', 'year', 'shawshank', 'redemption', '...      10   \n",
       "1       ['shawshank', 'redemption', 'without', 'doubt'...      10   \n",
       "2       ['believe', 'film', 'best', 'story', 'ever', '...       8   \n",
       "3       ['yes', 'spoilers', 'herethis', 'film', 'emoti...      10   \n",
       "4       ['heart', 'extraordinary', 'movie', 'brilliant...       8   \n",
       "...                                                   ...     ...   \n",
       "573908  ['go', 'wise', 'fast', 'pure', 'entertainment'...      10   \n",
       "573909  ['well', 'shall', 'say', 'ones', 'fun', 'rate'...       9   \n",
       "573910  ['go', 'best', 'movie', 'ever', 'seen', 'ive',...      10   \n",
       "573911  ['call', '1999', 'teenage', 'version', 'pulp',...       3   \n",
       "573912  ['movie', 'made', 'doubt', 'sucker', 'familyre...       2   \n",
       "\n",
       "                                       review_summary  \\\n",
       "0       A classic piece of unforgettable film-making.   \n",
       "1          Simply amazing. The best film of the 90's.   \n",
       "2                    The best story ever told on film   \n",
       "3                          Busy dying or busy living?   \n",
       "4              Great story, wondrously told and acted   \n",
       "...                                               ...   \n",
       "573908            The best teen movie of the nineties   \n",
       "573909                             Go - see the movie   \n",
       "573910             It's the best movie I've ever seen   \n",
       "573911                   Haven't we seen this before?   \n",
       "573912                         Go doesn't go anywhere   \n",
       "\n",
       "                                                  bigrams  \\\n",
       "0       [('oscar', 'year'), ('year', 'shawshank'), ('s...   \n",
       "1       [('shawshank', 'redemption'), ('redemption', '...   \n",
       "2       [('believe', 'film'), ('film', 'best'), ('best...   \n",
       "3       [('yes', 'spoilers'), ('spoilers', 'herethis')...   \n",
       "4       [('heart', 'extraordinary'), ('extraordinary',...   \n",
       "...                                                   ...   \n",
       "573908  [('go', 'wise'), ('wise', 'fast'), ('fast', 'p...   \n",
       "573909  [('well', 'shall'), ('shall', 'say'), ('say', ...   \n",
       "573910  [('go', 'best'), ('best', 'movie'), ('movie', ...   \n",
       "573911  [('call', '1999'), ('1999', 'teenage'), ('teen...   \n",
       "573912  [('movie', 'made'), ('made', 'doubt'), ('doubt...   \n",
       "\n",
       "                                              bigram_text  \n",
       "0       oscar year year shawshank shawshank redemption...  \n",
       "1       shawshank redemption redemption without withou...  \n",
       "2       believe film film best best story story ever e...  \n",
       "3       yes spoilers spoilers herethis herethis film f...  \n",
       "4       heart extraordinary extraordinary movie movie ...  \n",
       "...                                                   ...  \n",
       "573908  go wise wise fast fast pure pure entertainment...  \n",
       "573909  well shall shall say say ones ones fun fun rat...  \n",
       "573910  go best best movie movie ever ever seen seen i...  \n",
       "573911  call 1999 1999 teenage teenage version version...  \n",
       "573912  movie made made doubt doubt sucker sucker fami...  \n",
       "\n",
       "[573909 rows x 10 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_reviews_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = matrix\n",
    "y = imdb_reviews_bigrams['is_spoiler']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize SelectKBest with chi-squared as the score function\n",
    "k_best_selector = SelectKBest(score_func=chi2, k=100) #can change k accordingly\n",
    "\n",
    "X_train_selected = k_best_selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = k_best_selector.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n",
      "Confusion Matrix:\n",
      "[[81065  3584]\n",
      " [24075  6058]]\n",
      "F1 Score: 0.30\n"
     ]
    }
   ],
   "source": [
    "#logistic regression model\n",
    "log_model = LogisticRegression(max_iter=1000)\n",
    "log_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Predict using the trained model on the selected test set\n",
    "y_pred = log_model.predict(X_test_selected)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1score = f1_score(y_test, y_pred)\n",
    "print(f\"F1 Score: {f1score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.75\n",
      "SVM Confusion Matrix:\n",
      "[[82606  2043]\n",
      " [26349  3784]]\n",
      "SVM F1 Score: 0.21\n"
     ]
    }
   ],
   "source": [
    "#SVM model\n",
    "# Initialize and train SVM with selected features\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Predict using the trained SVM model on the selected test set\n",
    "y_pred_svm = svm_model.predict(X_test_selected)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {accuracy_svm:.2f}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "print(\"SVM Confusion Matrix:\")\n",
    "print(conf_matrix_svm)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1score_svm = f1_score(y_test, y_pred_svm)\n",
    "print(f\"SVM F1 Score: {f1score_svm:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.76\n",
      "Random Forest Confusion Matrix:\n",
      "[[80980  3669]\n",
      " [23900  6233]]\n",
      "Random Forest F1 Score: 0.31\n"
     ]
    }
   ],
   "source": [
    "#randomforest model\n",
    "# Initialize Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust n_estimators and other hyperparameters\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Predict using the trained Random Forest model on the test set\n",
    "X_test_selected = k_best_selector.transform(X_test)  # Transform the test set using the same feature selection\n",
    "y_pred_rf = rf_model.predict(X_test_selected)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf:.2f}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "print(\"Random Forest Confusion Matrix:\")\n",
    "print(conf_matrix_rf)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1score_rf = f1_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest F1 Score: {f1score_rf:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "Accuracy is the proportion of correctly classified instances. While Random Forest, logistic regression and SVM have similar accuracies (around 75-76%), it's important to consider other metrics for a more complete picture.\n",
    "\n",
    "F1 Score:\n",
    "F1 score is the harmonic mean of precision and recall. It's a good metric when there's an imbalance in the class distribution. In your case, Random Forest has a slightly higher F1 score (0.31) compared to SVM (0.21) and logistic regression (0.30)\n",
    "\n",
    "Conclusion:\n",
    "Logistic Regression and Random Forest perform similarly, with a slight advantage in F1 score for Random Forest.\n",
    "SVM, while having a comparable accuracy, lags behind in terms of F1 score, indicating a trade-off between precision and recall.\n",
    "Consider the specific goals of your application. If a balanced trade-off between precision and recall is crucial, Random Forest may be a reasonable choice. If interpretability is a priority, Logistic Regression might be preferred.\n",
    "Further model tuning, feature engineering, or exploring advanced techniques (such as deep learning) could potentially improve the performance of your models.\n",
    "\n",
    "\n",
    "We will be focusing on lstm as it is effective at capturing complex relationships in sequential data.\n",
    "It requires alot of data which in this case we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm (1/2)\n",
    "X = imdb_reviews_bigrams['bigram_text']\n",
    "y = imdb_reviews_bigrams['is_spoiler']\n",
    "\n",
    "#split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#vectorize\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features based on your data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Feature selection using SelectKBest on training set\n",
    "k_best_selector = SelectKBest(score_func=chi2, k=100)  # You can adjust k based on your data\n",
    "X_train_selected = k_best_selector.fit_transform(X_train_tfidf, y_train)\n",
    "X_test_selected = k_best_selector.transform(X_test_tfidf)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "# Convert labels to binary format\n",
    "y_train_binary = y_train.astype('int')\n",
    "y_test_binary = y_test.astype('int')\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "max_words = 10000  # You can adjust based on your vocabulary size\n",
    "max_len = 50 # You can adjust based on the desired sequence length\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_len, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6457/6457 [==============================] - 385s 59ms/step - loss: 0.5558 - accuracy: 0.7446 - val_loss: 0.5516 - val_accuracy: 0.7468\n",
      "Epoch 2/10\n",
      "6457/6457 [==============================] - 352s 55ms/step - loss: 0.5397 - accuracy: 0.7489 - val_loss: 0.5461 - val_accuracy: 0.7473\n",
      "Epoch 3/10\n",
      "6457/6457 [==============================] - 343s 53ms/step - loss: 0.5309 - accuracy: 0.7508 - val_loss: 0.5484 - val_accuracy: 0.7467\n",
      "Epoch 4/10\n",
      "6457/6457 [==============================] - 336s 52ms/step - loss: 0.5213 - accuracy: 0.7545 - val_loss: 0.5544 - val_accuracy: 0.7387\n",
      "Epoch 5/10\n",
      "6457/6457 [==============================] - 342s 53ms/step - loss: 0.5091 - accuracy: 0.7602 - val_loss: 0.5606 - val_accuracy: 0.7363\n",
      "3587/3587 [==============================] - 36s 10ms/step\n",
      "Test Accuracy: 0.7487\n",
      "Confusion Matrix:\n",
      "[[83352  1297]\n",
      " [27552  2581]]\n",
      "F1 Score: 0.1518\n"
     ]
    }
   ],
   "source": [
    "#lstm model(2/2)\n",
    "# Build the LSTM model\n",
    "embedding_dim = 50  # You can adjust based on your choice\n",
    "lstm_units = 100  # You can adjust based on your choice\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
    "model.add(LSTM(units=lstm_units))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_padded, y_train_binary, epochs=10, batch_size=64, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = (model.predict(X_test_padded) > 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_binary, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_binary, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1score = f1_score(y_test_binary, y_pred)\n",
    "print(f\"F1 Score: {f1score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for a high accuracy but a low F1 score in the context of the LSTM model could be attributed to class imbalance. F1 score takes into account both precision and recall, and it's particularly sensitive to false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_spoiler\n",
      "0    422986\n",
      "1    150923\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#check for class imbalance\n",
    "count_classes = imdb_reviews_bigrams['is_spoiler'].value_counts()\n",
    "print(count_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm addressing imbalance class(1/2)\n",
    "X = imdb_reviews_bigrams['bigram_text']\n",
    "y = imdb_reviews_bigrams['is_spoiler']\n",
    "\n",
    "#split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "#vectorize\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features based on your data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Feature selection using SelectKBest on training set\n",
    "k_best_selector = SelectKBest(score_func=chi2, k=100)  # You can adjust k based on your data\n",
    "X_train_selected = k_best_selector.fit_transform(X_train_tfidf, y_train)\n",
    "X_test_selected = k_best_selector.transform(X_test_tfidf)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "# Convert labels to binary format\n",
    "y_train_binary = y_train.astype('int')\n",
    "y_test_binary = y_test.astype('int')\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "max_words = 10000  # You can adjust based on your vocabulary size\n",
    "max_len = 50 # You can adjust based on the desired sequence length\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_len, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6457/6457 [==============================] - 614s 95ms/step - loss: 0.6620 - accuracy: 0.5864 - val_loss: 0.6568 - val_accuracy: 0.5848\n",
      "Epoch 2/10\n",
      "6457/6457 [==============================] - 391s 61ms/step - loss: 0.6418 - accuracy: 0.6142 - val_loss: 0.6576 - val_accuracy: 0.5940\n",
      "Epoch 3/10\n",
      "6457/6457 [==============================] - 411s 64ms/step - loss: 0.6216 - accuracy: 0.6350 - val_loss: 0.6553 - val_accuracy: 0.5982\n",
      "Epoch 4/10\n",
      "6457/6457 [==============================] - 474s 73ms/step - loss: 0.5970 - accuracy: 0.6590 - val_loss: 0.6625 - val_accuracy: 0.6012\n",
      "Epoch 5/10\n",
      "6457/6457 [==============================] - 385s 60ms/step - loss: 0.5716 - accuracy: 0.6788 - val_loss: 0.6838 - val_accuracy: 0.5988\n",
      "Epoch 6/10\n",
      "6457/6457 [==============================] - 400s 62ms/step - loss: 0.5464 - accuracy: 0.6977 - val_loss: 0.7154 - val_accuracy: 0.5816\n",
      "3587/3587 [==============================] - 32s 9ms/step\n",
      "Test Accuracy: 0.5986\n",
      "Confusion Matrix:\n",
      "[[51370 33279]\n",
      " [12789 17344]]\n",
      "F1 Score: 0.4295\n"
     ]
    }
   ],
   "source": [
    "#lstm addressing imbalance class(2/2)\n",
    "# Build the LSTM model\n",
    "embedding_dim = 50  # You can adjust based on your choice\n",
    "lstm_units = 100  # You can adjust based on your choice\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
    "model.add(LSTM(units=lstm_units))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with class weights\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with class weights\n",
    "history = model.fit(X_train_padded, y_train_binary, epochs=10, batch_size=64, validation_split=0.1, callbacks=[early_stopping], class_weight=class_weights_dict)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = (model.predict(X_test_padded) > 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_binary, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_binary, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1score = f1_score(y_test_binary, y_pred)\n",
    "print(f\"F1 Score: {f1score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the context of detecting movie spoilers, where the classes are imbalanced, optimizing for F1 score might be more relevant. F1 score considers both precision and recall, providing a better balance, especially when false positives and false negatives carry different costs. Drop in accuracy is accompanied by an improvement in F1 score and better performance on the minority class (spoilers in this case), it could be considered a worthwhile trade-off. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6457/6457 [==============================] - 1571s 243ms/step - loss: 0.6593 - accuracy: 0.5967 - val_loss: 0.6652 - val_accuracy: 0.5777\n",
      "Epoch 2/10\n",
      "6457/6457 [==============================] - 2690s 417ms/step - loss: 0.6374 - accuracy: 0.6260 - val_loss: 0.6526 - val_accuracy: 0.5985\n",
      "Epoch 3/10\n",
      "6457/6457 [==============================] - 2928s 453ms/step - loss: 0.6148 - accuracy: 0.6502 - val_loss: 0.6537 - val_accuracy: 0.5963\n",
      "Epoch 4/10\n",
      "6457/6457 [==============================] - 2524s 391ms/step - loss: 0.5895 - accuracy: 0.6737 - val_loss: 0.6945 - val_accuracy: 0.5693\n",
      "Epoch 5/10\n",
      "6457/6457 [==============================] - 2614s 405ms/step - loss: 0.5618 - accuracy: 0.6946 - val_loss: 0.6648 - val_accuracy: 0.6211\n",
      "3587/3587 [==============================] - 159s 44ms/step\n",
      "Test Accuracy: 0.5980\n",
      "Confusion Matrix:\n",
      "[[50507 34142]\n",
      " [12000 18133]]\n",
      "F1 Score: 0.4401\n"
     ]
    }
   ],
   "source": [
    "#Improving accuracy for lstm after addressing class imbalance\n",
    "# Build the Bidirectional LSTM model\n",
    "embedding_dim = 50\n",
    "lstm_units = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(units=lstm_units)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with class weights\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with class weights\n",
    "history = model.fit(X_train_padded, y_train_binary, epochs=10, batch_size=64, validation_split=0.1, callbacks=[early_stopping], class_weight=class_weights_dict)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = (model.predict(X_test_padded) > 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_binary, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_binary, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1score = f1_score(y_test_binary, y_pred)\n",
    "print(f\"F1 Score: {f1score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Bidirectional LSTMs come with additional hyperparameters, such as the number of units in each direction, dropout rates, etc. If these hyperparameters are not tuned properly, they might negatively impact the performance of the bidirectional model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "3049/3049 [==============================] - 4036s 1s/step - loss: 0.6587 - accuracy: 0.5965 - val_loss: 0.6615 - val_accuracy: 0.5887\n",
      "Epoch 2/15\n",
      "3049/3049 [==============================] - 4469s 1s/step - loss: 0.6389 - accuracy: 0.6279 - val_loss: 0.6527 - val_accuracy: 0.5945\n",
      "Epoch 3/15\n",
      "3049/3049 [==============================] - 3064s 1s/step - loss: 0.6200 - accuracy: 0.6470 - val_loss: 0.6528 - val_accuracy: 0.6010\n",
      "Epoch 4/15\n",
      "3049/3049 [==============================] - 3537s 1s/step - loss: 0.5983 - accuracy: 0.6653 - val_loss: 0.6484 - val_accuracy: 0.6174\n",
      "Epoch 5/15\n",
      "3049/3049 [==============================] - 4192s 1s/step - loss: 0.5757 - accuracy: 0.6825 - val_loss: 0.6532 - val_accuracy: 0.6212\n",
      "Epoch 6/15\n",
      "3049/3049 [==============================] - 4204s 1s/step - loss: 0.5519 - accuracy: 0.6995 - val_loss: 0.7088 - val_accuracy: 0.5841\n",
      "Epoch 7/15\n",
      "3049/3049 [==============================] - 4472s 1s/step - loss: 0.5258 - accuracy: 0.7170 - val_loss: 0.7213 - val_accuracy: 0.5908\n",
      "3587/3587 [==============================] - 298s 83ms/step\n",
      "Test Accuracy: 0.6185\n",
      "Confusion Matrix:\n",
      "[[55507 29142]\n",
      " [14644 15489]]\n",
      "F1 Score: 0.4143\n"
     ]
    }
   ],
   "source": [
    "#fine tuning the hyperparameters of the models above\n",
    "#Improving accuracy for lstm after addressing class imbalance\n",
    "# Build the Bidirectional LSTM model\n",
    "embedding_dim = 64 #50 -> 64\n",
    "lstm_units = 128 #100 -> 128 \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(units=lstm_units)))\n",
    "model.add(Dropout(0.4)) #0.5 -> 0.4\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with class weights\n",
    "optimizer = Adam(learning_rate=0.0005) #decrease learning rate\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with class weights\n",
    "history = model.fit(X_train_padded, y_train_binary, epochs=15, batch_size=128, validation_split=0.15, callbacks=[early_stopping], class_weight=class_weights_dict) #epochs from 10, batch size from 64 and validation from 0.1\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = (model.predict(X_test_padded) > 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_binary, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_binary, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1score = f1_score(y_test_binary, y_pred)\n",
    "print(f\"F1 Score: {f1score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6098/6098 [==============================] - 1807s 296ms/step - loss: 0.6587 - accuracy: 0.6001 - val_loss: 0.6413 - val_accuracy: 0.6209\n",
      "Epoch 2/20\n",
      "6098/6098 [==============================] - 1655s 271ms/step - loss: 0.6360 - accuracy: 0.6305 - val_loss: 0.6309 - val_accuracy: 0.6329\n",
      "Epoch 3/20\n",
      "6098/6098 [==============================] - 1663s 273ms/step - loss: 0.6112 - accuracy: 0.6561 - val_loss: 0.6573 - val_accuracy: 0.6001\n",
      "Epoch 4/20\n",
      "6098/6098 [==============================] - 1635s 268ms/step - loss: 0.5827 - accuracy: 0.6800 - val_loss: 0.6758 - val_accuracy: 0.5990\n",
      "Epoch 5/20\n",
      "6098/6098 [==============================] - 1740s 285ms/step - loss: 0.5508 - accuracy: 0.7052 - val_loss: 0.6627 - val_accuracy: 0.6281\n",
      "3587/3587 [==============================] - 256s 71ms/step\n",
      "Test Accuracy: 0.6327\n",
      "Confusion Matrix:\n",
      "[[57002 27647]\n",
      " [14508 15625]]\n",
      "F1 Score: 0.4257\n"
     ]
    }
   ],
   "source": [
    "#fine tuning the hyperparameters of the models\n",
    "#Improving accuracy for lstm after addressing class imbalance\n",
    "# Build the Bidirectional LSTM model\n",
    "embedding_dim = 128 #64 -> 128\n",
    "lstm_units = 256 #128 -> 256\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(units=lstm_units)))\n",
    "model.add(Dropout(0.4)) #0.5 -> 0.4\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with class weights\n",
    "optimizer = Adam(learning_rate=0.0005) #decrease learning rate\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with class weights\n",
    "history = model.fit(X_train_padded, y_train_binary, epochs=20, batch_size=64, validation_split=0.15, callbacks=[early_stopping], class_weight=class_weights_dict) #epochs from 10, batch size from 64 and validation from 0.1\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = (model.predict(X_test_padded) > 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_binary, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_binary, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1score = f1_score(y_test_binary, y_pred)\n",
    "print(f\"F1 Score: {f1score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6098/6098 [==============================] - 2345s 384ms/step - loss: 0.6584 - accuracy: 0.5998 - val_loss: 0.6348 - val_accuracy: 0.6478\n",
      "Epoch 2/20\n",
      "6098/6098 [==============================] - 2275s 373ms/step - loss: 0.6340 - accuracy: 0.6317 - val_loss: 0.6466 - val_accuracy: 0.6215\n",
      "Epoch 3/20\n",
      "6098/6098 [==============================] - 2358s 387ms/step - loss: 0.6039 - accuracy: 0.6614 - val_loss: 0.6727 - val_accuracy: 0.5712\n",
      "Epoch 4/20\n",
      "6098/6098 [==============================] - 2302s 378ms/step - loss: 0.5698 - accuracy: 0.6902 - val_loss: 0.7011 - val_accuracy: 0.5677\n",
      "3587/3587 [==============================] - 288s 80ms/step\n",
      "Test Accuracy: 0.6485\n",
      "Confusion Matrix:\n",
      "[[59731 24918]\n",
      " [15427 14706]]\n",
      "F1 Score: 0.4216\n"
     ]
    }
   ],
   "source": [
    "#fine tuning the hyperparameters of the models\n",
    "#Improving accuracy for lstm after addressing class imbalance\n",
    "# Build the Bidirectional LSTM model\n",
    "embedding_dim = 256 #64 -> 128\n",
    "lstm_units = 256 #128 -> 256\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(units=lstm_units)))\n",
    "model.add(Dropout(0.4)) #0.5 -> 0.4\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with class weights\n",
    "optimizer = Adam(learning_rate=0.0005) #decrease learning rate\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with class weights\n",
    "history = model.fit(X_train_padded, y_train_binary, epochs=20, batch_size=64, validation_split=0.15, callbacks=[early_stopping], class_weight=class_weights_dict) #epochs from 10, batch size from 64 and validation from 0.1\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = (model.predict(X_test_padded) > 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_binary, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_binary, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1score = f1_score(y_test_binary, y_pred)\n",
    "print(f\"F1 Score: {f1score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6098/6098 [==============================] - 8045s 1s/step - loss: 0.6594 - accuracy: 0.5990 - val_loss: 0.6339 - val_accuracy: 0.6333\n",
      "Epoch 2/20\n",
      "6098/6098 [==============================] - 7758s 1s/step - loss: 0.6362 - accuracy: 0.6295 - val_loss: 0.6315 - val_accuracy: 0.6373\n",
      "Epoch 3/20\n",
      "6098/6098 [==============================] - 7997s 1s/step - loss: 0.6084 - accuracy: 0.6579 - val_loss: 0.6457 - val_accuracy: 0.6143\n",
      "Epoch 4/20\n",
      "6098/6098 [==============================] - 7858s 1s/step - loss: 0.5727 - accuracy: 0.6884 - val_loss: 0.6759 - val_accuracy: 0.6069\n",
      "Epoch 5/20\n",
      "6098/6098 [==============================] - 7934s 1s/step - loss: 0.5306 - accuracy: 0.7190 - val_loss: 0.6940 - val_accuracy: 0.6007\n",
      "3587/3587 [==============================] - 625s 174ms/step\n",
      "Test Accuracy: 0.6385\n",
      "Confusion Matrix:\n",
      "[[58118 26531]\n",
      " [14967 15166]]\n",
      "F1 Score: 0.4223\n"
     ]
    }
   ],
   "source": [
    "#fine tuning the hyperparameters of the models by adding additional lstm layer\n",
    "#Improving accuracy for lstm after addressing class imbalance\n",
    "# Build the Bidirectional LSTM model\n",
    "embedding_dim = 256 #64 -> 128\n",
    "lstm_units = 256 #128 -> 256\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(units=lstm_units,return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(units=lstm_units))) #additional layer\n",
    "model.add(Dropout(0.4)) #0.5 -> 0.4\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with class weights\n",
    "optimizer = Adam(learning_rate=0.0005) #decrease learning rate\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with class weights\n",
    "history = model.fit(X_train_padded, y_train_binary, epochs=20, batch_size=64, validation_split=0.15, callbacks=[early_stopping], class_weight=class_weights_dict) #epochs from 10, batch size from 64 and validation from 0.1\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = (model.predict(X_test_padded) > 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_binary, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_binary, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1score = f1_score(y_test_binary, y_pred)\n",
    "print(f\"F1 Score: {f1score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional layers can lead to overfitting, especially if the model is too complex for the given dataset. Overfitting occurs when the model learns the training data too well, including its noise and outliers, and fails to generalize to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6098/6098 [==============================] - 4689s 768ms/step - loss: 0.7617 - accuracy: 0.5725 - val_loss: 0.6632 - val_accuracy: 0.5989\n",
      "Epoch 2/20\n",
      "6098/6098 [==============================] - 4418s 725ms/step - loss: 0.6571 - accuracy: 0.6147 - val_loss: 0.6808 - val_accuracy: 0.5785\n",
      "Epoch 3/20\n",
      "6098/6098 [==============================] - 4305s 706ms/step - loss: 0.6499 - accuracy: 0.6267 - val_loss: 0.6951 - val_accuracy: 0.5426\n",
      "Epoch 4/20\n",
      "6098/6098 [==============================] - 4189s 687ms/step - loss: 0.6451 - accuracy: 0.6334 - val_loss: 0.6542 - val_accuracy: 0.6289\n",
      "Epoch 5/20\n",
      "6098/6098 [==============================] - 4382s 719ms/step - loss: 0.6408 - accuracy: 0.6402 - val_loss: 0.6758 - val_accuracy: 0.5920\n",
      "Epoch 6/20\n",
      "6098/6098 [==============================] - 4178s 685ms/step - loss: 0.6372 - accuracy: 0.6443 - val_loss: 0.6540 - val_accuracy: 0.6198\n",
      "Epoch 7/20\n",
      "6098/6098 [==============================] - 4130s 677ms/step - loss: 0.6341 - accuracy: 0.6481 - val_loss: 0.7062 - val_accuracy: 0.5505\n",
      "Epoch 8/20\n",
      "6098/6098 [==============================] - 4473s 734ms/step - loss: 0.6323 - accuracy: 0.6507 - val_loss: 0.6808 - val_accuracy: 0.5880\n",
      "Epoch 9/20\n",
      "6098/6098 [==============================] - 4311s 707ms/step - loss: 0.6307 - accuracy: 0.6525 - val_loss: 0.6842 - val_accuracy: 0.5831\n",
      "3587/3587 [==============================] - 321s 89ms/step\n",
      "Test Accuracy: 0.6211\n",
      "Confusion Matrix:\n",
      "[[55317 29332]\n",
      " [14161 15972]]\n",
      "F1 Score: 0.4235\n"
     ]
    }
   ],
   "source": [
    "#fine tuning the hyperparameters of the models with regularisation\n",
    "#Improving accuracy for lstm after addressing class imbalance\n",
    "# Build the Bidirectional LSTM model\n",
    "embedding_dim = 256 #64 -> 128\n",
    "lstm_units = 256 #128 -> 256\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(units=lstm_units, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01)))) #additional layer + regularisation\n",
    "model.add(Dropout(0.6)) #0.5 -> 0.6\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with class weights\n",
    "optimizer = Adam(learning_rate=0.0005) #decrease learning rate\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with class weights\n",
    "history = model.fit(X_train_padded, y_train_binary, epochs=20, batch_size=64, validation_split=0.15, callbacks=[early_stopping], class_weight=class_weights_dict) #epochs from 10, batch size from 64 and validation from 0.1\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = (model.predict(X_test_padded) > 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_binary, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_binary, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1score = f1_score(y_test_binary, y_pred)\n",
    "print(f\"F1 Score: {f1score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strength of the regularization may be too high, causing the model to penalize the weights too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6098/6098 [==============================] - 5889s 965ms/step - loss: 0.6768 - accuracy: 0.5861 - val_loss: 0.6712 - val_accuracy: 0.5802\n",
      "Epoch 2/20\n",
      "6098/6098 [==============================] - 3911s 641ms/step - loss: 0.6509 - accuracy: 0.6204 - val_loss: 0.6418 - val_accuracy: 0.6335\n",
      "Epoch 3/20\n",
      "6098/6098 [==============================] - 22367s 4s/step - loss: 0.6444 - accuracy: 0.6321 - val_loss: 0.6601 - val_accuracy: 0.6068\n",
      "Epoch 4/20\n",
      "6098/6098 [==============================] - 4199s 689ms/step - loss: 0.6375 - accuracy: 0.6425 - val_loss: 0.6682 - val_accuracy: 0.5947\n",
      "Epoch 5/20\n",
      "6098/6098 [==============================] - 5457s 895ms/step - loss: 0.6312 - accuracy: 0.6502 - val_loss: 0.6649 - val_accuracy: 0.6028\n",
      "3587/3587 [==============================] - 297s 82ms/step\n",
      "Test Accuracy: 0.6340\n",
      "Confusion Matrix:\n",
      "[[57328 27321]\n",
      " [14694 15439]]\n",
      "F1 Score: 0.4236\n"
     ]
    }
   ],
   "source": [
    "#fine tuning the hyperparameters of the models with regularisation\n",
    "#Improving accuracy for lstm after addressing class imbalance\n",
    "# Build the Bidirectional LSTM model\n",
    "embedding_dim = 256 #64 -> 128\n",
    "lstm_units = 256 #128 -> 256\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(units=lstm_units, kernel_regularizer=l2(0.001), recurrent_regularizer=l2(0.001)))) #additional layer + regularisation adjusted\n",
    "model.add(Dropout(0.6)) #0.5 -> 0.6\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with class weights\n",
    "optimizer = Adam(learning_rate=0.0005) #decrease learning rate\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with class weights\n",
    "history = model.fit(X_train_padded, y_train_binary, epochs=20, batch_size=64, validation_split=0.15, callbacks=[early_stopping], class_weight=class_weights_dict) #epochs from 10, batch size from 64 and validation from 0.1\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = (model.predict(X_test_padded) > 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_binary, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_binary, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1score = f1_score(y_test_binary, y_pred)\n",
    "print(f\"F1 Score: {f1score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#bert (1/2)\n",
    "X = imdb_reviews_bigrams['bigram_text']\n",
    "y = imdb_reviews_bigrams['is_spoiler']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "max_len = 50  # You can adjust the maximum sequence length\n",
    "X_train_tokenized = tokenizer(list(X_train), padding=True, truncation=True, return_tensors='pt', max_length=max_len)\n",
    "X_test_tokenized = tokenizer(list(X_test), padding=True, truncation=True, return_tensors='pt', max_length=max_len)\n",
    "\n",
    "# Convert labels to torch tensors\n",
    "y_train_tensor = torch.tensor(y_train.values)\n",
    "y_test_tensor = torch.tensor(y_test.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
